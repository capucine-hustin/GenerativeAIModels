{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941cb1c7-4983-4578-9995-411508e9e81b",
   "metadata": {},
   "source": [
    "This file builds an MNIST image classifier using the pre-trained Vision Transformer (google/vit-base-patch16-224) from Hugging Face and fine-tunes it with the actual MNIST dataset to achieve 99.6% accuracy. The purpose of this MNIST image classifier is to provide a quantitative performance measurement for different generative models we have built so far, namely GANs, PixelCNN, VAE, and diffusion models.\n",
    "\n",
    "## Procedures\n",
    "1. We start with the `vit-base-patch16-224` model from Hugging Face, designed for image classification with a resolution of 224x224 pixels.\n",
    "2. The model is further fine-tuned on the original MNIST dataset.\n",
    "3. After fine-tuning, we evaluate the model on the MNIST test set to assess its accuracy, achieving approximately 99.6% accuracy.\n",
    "4. We generate 5000 new reconstructed MNIST images with their original labels using the Variational Autoencoder (VAE) model.\n",
    "5. The fine-tuned model is used as a 'human judge' to evaluate how well the reconstructed MNIST images generated from the VAE model can be correctly classified by the fine-tuned Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613a1497-a8c7-45de-883a-dd0f216238da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 01:59:09.020536: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-25 01:59:09.049357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 01:59:09.603569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Lambda\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torchmetrics import Accuracy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "# Set a seed value\n",
    "seed = 100 \n",
    "random.seed(seed)  \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd28a86-f43d-41af-bef5-06a2be1f301f",
   "metadata": {},
   "source": [
    "### prepare MNIST data to dataloader and load pre-train vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a87dfa5-c5fc-42a1-8ca3-b026e95cdb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=10, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to fit the model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    Lambda(lambda x: x.repeat(3, 1, 1))  # Repeat the grayscale channel to simulate RGB\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load a pre-trained Vision Transformer with a specific configuration for 10 classes\n",
    "config = ViTConfig.from_pretrained('google/vit-base-patch16-224', num_labels=10)\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Reinitialize the classifier layer as it was mismatched\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 10)  # Ensure the input features match hidden_size of model\n",
    "model.classifier.to(device)\n",
    "\n",
    "# Verify and replace the classifier if needed\n",
    "if model.classifier.out_features != 10:\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, 10)\n",
    "model.classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0d81c-ae85-41fa-a074-a5fe4c52c7d4",
   "metadata": {},
   "source": [
    "### check the model performance without fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352624e3-935b-461b-a894-fb72f34edede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST test data (without fine-tuning): 0.1360\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    # Move model to the right device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_preds, total_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            # Ensure data is on the same device as the model\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=images)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "    return accuracy\n",
    "    \n",
    "# Assuming 'device' is defined (e.g., device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# Run evaluation\n",
    "test_accuracy = evaluate_model(model, test_loader, device)\n",
    "print(f\"Accuracy on MNIST test data (without fine-tuning): {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5510b57-bae0-4a16-9dac-ddc17c6b70d0",
   "metadata": {},
   "source": [
    "### train vision transformer on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b03203-c17a-40e7-857e-dc82c85bc6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.002542877569794655\n",
      "Epoch 2: Loss 0.001039237598888576\n",
      "Epoch 3: Loss 0.0011476653162389994\n",
      "Epoch 4: Loss 0.00024169111566152424\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Prepare learning rate scheduler\n",
    "num_epochs = 4\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Update model's classifier head if necessary (for MNIST, 10 classes)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 10)\n",
    "model.classifier.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # The transformers model expects pixel_values key in the input dictionary\n",
    "        outputs = model(pixel_values=images, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3230072-fefc-485b-8b7f-eb7b764722ac",
   "metadata": {},
   "source": [
    "### compute the test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae47a1a-d2e5-4807-abd8-d84a42860251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.996269702911377\n"
     ]
    }
   ],
   "source": [
    "# Setup the metric\n",
    "accuracy_metric = Accuracy(num_classes=10, average='macro', task='multiclass').to(device)\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "total_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=images)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        total_accuracy += accuracy_metric(predictions, labels)\n",
    "\n",
    "# Calculate the average accuracy\n",
    "average_accuracy = total_accuracy / len(test_loader)\n",
    "print(f\"Test Accuracy: {average_accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4bcec-7bf6-4938-89d4-4f719ea3a255",
   "metadata": {},
   "source": [
    "### load reconstructed data into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338cfa1c-096c-4287-8927-d8fe46982fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# try some augmented data from VAE (5000 images)\n",
    "\n",
    "class ReconstructedDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the reconstructed images and labels.csv.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labels = pd.read_csv(os.path.join(root_dir, 'labels.csv'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, f'reconstructed_{idx}.png')\n",
    "        image = Image.open(img_name).convert('RGB')  \n",
    "        label = int(self.labels.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transformations for the Vision Transformer\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match the model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Assuming reconstructed images are saved in 'reconstructed_images' directory\n",
    "reconstructed_dataset = ReconstructedDataset(root_dir='reconstructed_images', transform=transform)\n",
    "reconstructed_loader = DataLoader(reconstructed_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "images, _ = next(iter(reconstructed_loader))\n",
    "print(images.shape)  # Should output torch.Size([batch_size, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6e0ce-1407-4686-93dd-c0f6e886c5b4",
   "metadata": {},
   "source": [
    "### compute accuracy for the VAE generated reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3e7864-01e7-4315-90ec-181823f7cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on reconstructed images: 0.7070\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_preds, total_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(total_labels, total_preds)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(model, reconstructed_loader, device)\n",
    "print(f\"Accuracy on reconstructed images: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d4cf6-7388-4893-9f09-e6819f2b6ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
