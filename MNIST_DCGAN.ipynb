{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfsiv_WbBchx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BanLUSQ2BrOW",
        "outputId": "6e3f4b7f-5d57-44f6-e168-d80b398c8bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5119845.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134320.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1275209.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5383025.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv0 = nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.conv0_drop = nn.Dropout2d(0.25)\n",
        "        self.conv1 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv1_drop = nn.Dropout2d(0.25)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv2_drop = nn.Dropout2d(0.25)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.conv3_drop = nn.Dropout2d(0.25)\n",
        "        self.fc = nn.Linear(12544, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = F.leaky_relu(self.conv0(x), 0.2)\n",
        "        x = self.conv0_drop(x)\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = self.conv1_drop(x)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = self.conv2_drop(x)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = self.conv3_drop(x)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "\n",
        "        return num_features"
      ],
      "metadata": {
        "id": "Oet6O6tvBxvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(100, 256*7*7)\n",
        "        self.trans_conv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
        "        self.trans_conv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.trans_conv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.trans_conv4 = nn.ConvTranspose2d(32, 1, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, 256, 7, 7)\n",
        "        x = F.relu(self.trans_conv1(x))\n",
        "        x = F.relu(self.trans_conv2(x))\n",
        "        x = F.relu(self.trans_conv3(x))\n",
        "        x = self.trans_conv4(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kP2ieOLbByuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device using: {device}\")\n",
        "D = Discriminator()\n",
        "G = Generator()\n",
        "D.to(device)\n",
        "G.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sDdTSlNosOb",
        "outputId": "1c466138-97ca-412f-a32d-53d72e4d45d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device using: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (fc): Linear(in_features=100, out_features=12544, bias=True)\n",
              "  (trans_conv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  (trans_conv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (trans_conv3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (trans_conv4): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noise(batch_size, dim):\n",
        "    return torch.rand(batch_size, dim) * 2 - 1"
      ],
      "metadata": {
        "id": "dOyTcgK7o3Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def discriminator_real_loss(real_out):\n",
        "    real_label = torch.ones(real_out.size()[0], 1).to(device)\n",
        "    real_loss = Loss(real_out.squeeze(), real_label.squeeze())\n",
        "    return real_loss\n",
        "\n",
        "def discriminator_fake_loss(fake_out):\n",
        "    fake_label = torch.zeros(fake_out.size()[0], 1).to(device)\n",
        "    fake_loss = Loss(fake_out.squeeze(), fake_label.squeeze())\n",
        "    return fake_loss\n",
        "\n",
        "def discriminator_loss(real_out, fake_out):\n",
        "    real_loss = discriminator_real_loss(real_out)\n",
        "    fake_loss = discriminator_fake_loss(fake_out)\n",
        "    total_loss = (real_loss + fake_loss)\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(gen_disc_out):\n",
        "    label = torch.ones(gen_disc_out.size()[0], 1).to(device)\n",
        "    gen_loss = Loss(gen_disc_out.squeeze(), label.squeeze())\n",
        "    return gen_loss"
      ],
      "metadata": {
        "id": "P9E0_VioCdog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_opt = optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "gen_opt = optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
      ],
      "metadata": {
        "id": "3pa9_wD_ChyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(D, G, disc_opt, gen_opt, train_dl, batch_size = 32, epochs = 25, gen_input_size = 100):\n",
        "\n",
        "    disc_losses = []\n",
        "    gen_losses = []\n",
        "    sample_size = 16\n",
        "    fixed_samples = generate_noise(sample_size, gen_input_size)\n",
        "    fixed_samples = fixed_samples.to(device)\n",
        "    D.train()\n",
        "    G.train()\n",
        "\n",
        "    for epoch in range(epochs + 1):\n",
        "        disc_loss_total = 0\n",
        "        gen_loss_total = 0\n",
        "        gen_out = 0\n",
        "        for train_x, y in train_dl:\n",
        "            disc_opt.zero_grad()\n",
        "            train_x = train_x*2 - 1\n",
        "            train_x = train_x.to(device)\n",
        "            real_out = D(train_x.float())\n",
        "            disc_gen_in = random_noise_generator(batch_size, gen_input_size)\n",
        "            disc_gen_in = disc_gen_in.to(device)\n",
        "            disc_gen_out = G(disc_gen_in.float()).detach()\n",
        "            fake_out = D(disc_gen_out.float())\n",
        "            disc_loss = discriminator_loss(real_out, fake_out)\n",
        "            disc_loss_total += disc_loss\n",
        "            disc_loss.backward()\n",
        "            disc_opt.step()\n",
        "            gen_opt.zero_grad()\n",
        "            gen_out = G(disc_gen_in.float())\n",
        "            gen_disc_out = D(gen_out.float())\n",
        "\n",
        "            gen_loss = generator_loss(gen_disc_out)\n",
        "            gen_loss_total += gen_loss\n",
        "            gen_loss.backward()\n",
        "            gen_opt.step()\n",
        "\n",
        "        disc_losses.append(disc_loss_total)\n",
        "        gen_losses.append(gen_loss_total)\n",
        "        print(\"Epoch \", epoch, \": Discriminator Loss = \", disc_loss_total/len(train_dl), \", Generator Loss = \", gen_loss_total/len(train_dl))\n",
        "    return disc_losses, gen_losses"
      ],
      "metadata": {
        "id": "a1O2dbtICnLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_losses, gen_losses = train(D, G, disc_opt, gen_opt, trainloader, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxUO97D3Co5H",
        "outputId": "2d6ccb30-df98-4592-bfb5-084dbf6ad88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0 : Discriminator Loss =  tensor(0.0120, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(11.5050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  1 : Discriminator Loss =  tensor(0.0037, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(14.7734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  2 : Discriminator Loss =  tensor(2.5528e-06, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(19.3135, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  3 : Discriminator Loss =  tensor(2.1893e-06, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(22.8452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  4 : Discriminator Loss =  tensor(0.0015, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(28.1267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  5 : Discriminator Loss =  tensor(2.4710e-06, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(31.7349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  6 : Discriminator Loss =  tensor(1.3560e-07, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(31.9718, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  7 : Discriminator Loss =  tensor(5.6452e-08, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(32.9542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  8 : Discriminator Loss =  tensor(5.5273e-09, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(36.5403, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  9 : Discriminator Loss =  tensor(1.2716e-09, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(40.3383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  10 : Discriminator Loss =  tensor(2.6425e-10, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(42.6469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  11 : Discriminator Loss =  tensor(0.0024, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(42.6430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  12 : Discriminator Loss =  tensor(1.6629e-09, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(47.6786, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  13 : Discriminator Loss =  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(53.2702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  14 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(68.2859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  15 : Discriminator Loss =  tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(84.3310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  16 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(97.9238, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  17 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(97.5877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  18 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(97.5515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  19 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(97.5301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  20 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(97.3142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  21 : Discriminator Loss =  tensor(7.8583e-06, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(97.3712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  22 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(92.7827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  23 : Discriminator Loss =  tensor(7.7486e-11, device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(112.6623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch  24 : Discriminator Loss =  tensor(0., device='cuda:0', grad_fn=<DivBackward0>) , Generator Loss =  tensor(113.3145, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    }
  ]
}